{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import utils\n",
    "import vae\n",
    "import u_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to ensure reproducible training/validation split\n",
    "random.seed(42)\n",
    "\n",
    "# directorys with data and to store training checkpoints and logs\n",
    "WORKING_DIR = Path(r\"C:\\Users\\marti\\OneDrive - TU Eindhoven\\Documenten\\Master\\Q3\\Capita Selecta\\Project\")\n",
    "DATA_DIR = WORKING_DIR / \"Data\"\n",
    "CHECKPOINTS_DIR = WORKING_DIR / \"vae_model_weights\"\n",
    "CHECKPOINTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PROGRESS_DIR = WORKING_DIR / \"progress\"\n",
    "PROGRESS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TENSORBOARD_LOGDIR = WORKING_DIR / \"vae_runs\"\n",
    "TENSORBOARD_LOGDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# training settings and hyperparameters\n",
    "NO_VALIDATION_PATIENTS = 2\n",
    "IMAGE_SIZE = [64, 64]\n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 200\n",
    "DECAY_LR_AFTER = 50\n",
    "LEARNING_RATE = 1e-3\n",
    "DISPLAY_FREQ = 5\n",
    "\n",
    "# dimension of VAE latent space\n",
    "Z_DIM = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WindowsPath('C:/Users/marti/OneDrive - TU Eindhoven/Documenten/Master/Q3/Capita Selecta/Project/Data/p120'), WindowsPath('C:/Users/marti/OneDrive - TU Eindhoven/Documenten/Master/Q3/Capita Selecta/Project/Data/p133'), WindowsPath('C:/Users/marti/OneDrive - TU Eindhoven/Documenten/Master/Q3/Capita Selecta/Project/Data/p119'), WindowsPath('C:/Users/marti/OneDrive - TU Eindhoven/Documenten/Master/Q3/Capita Selecta/Project/Data/p117'), WindowsPath('C:/Users/marti/OneDrive - TU Eindhoven/Documenten/Master/Q3/Capita Selecta/Project/Data/p135'), WindowsPath('C:/Users/marti/OneDrive - TU Eindhoven/Documenten/Master/Q3/Capita Selecta/Project/Data/p129'), WindowsPath('C:/Users/marti/OneDrive - TU Eindhoven/Documenten/Master/Q3/Capita Selecta/Project/Data/p116'), WindowsPath('C:/Users/marti/OneDrive - TU Eindhoven/Documenten/Master/Q3/Capita Selecta/Project/Data/p108'), WindowsPath('C:/Users/marti/OneDrive - TU Eindhoven/Documenten/Master/Q3/Capita Selecta/Project/Data/p125'), WindowsPath('C:/Users/marti/OneDrive - TU Eindhoven/Documenten/Master/Q3/Capita Selecta/Project/Data/p109'), WindowsPath('C:/Users/marti/OneDrive - TU Eindhoven/Documenten/Master/Q3/Capita Selecta/Project/Data/p115'), WindowsPath('C:/Users/marti/OneDrive - TU Eindhoven/Documenten/Master/Q3/Capita Selecta/Project/Data/p128'), WindowsPath('C:/Users/marti/OneDrive - TU Eindhoven/Documenten/Master/Q3/Capita Selecta/Project/Data/p102'), WindowsPath('C:/Users/marti/OneDrive - TU Eindhoven/Documenten/Master/Q3/Capita Selecta/Project/Data/p107'), WindowsPath('C:/Users/marti/OneDrive - TU Eindhoven/Documenten/Master/Q3/Capita Selecta/Project/Data/p127')]\n"
     ]
    }
   ],
   "source": [
    "# function to reduce the\n",
    "def lr_lambda(the_epoch):\n",
    "    \"\"\"Function for scheduling learning rate\"\"\"\n",
    "    return (\n",
    "        1.0\n",
    "        if the_epoch < DECAY_LR_AFTER\n",
    "        else 1 - float(the_epoch - DECAY_LR_AFTER) / (N_EPOCHS - DECAY_LR_AFTER)\n",
    ")\n",
    "\n",
    "\n",
    "# find patient folders in training directory\n",
    "# excluding hidden folders (start with .)\n",
    "patients = [\n",
    "    path\n",
    "    for path in DATA_DIR.glob(\"*\")\n",
    "    if not any(part.startswith(\".\") for part in path.parts)\n",
    "]\n",
    "random.shuffle(patients)\n",
    "\n",
    "# split in training/validation after shuffling\n",
    "partition = {\n",
    "    \"train\": patients[:-NO_VALIDATION_PATIENTS],\n",
    "    \"validation\": patients[-NO_VALIDATION_PATIENTS:],\n",
    "}\n",
    "\n",
    "# load training data and create DataLoader with batching and shuffling\n",
    "dataset = utils.ProstateMRDataset(partition[\"train\"], IMAGE_SIZE)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "# load validation data\n",
    "valid_dataset = utils.ProstateMRDataset(partition[\"validation\"], IMAGE_SIZE)\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "# initialise model, optimiser\n",
    "vae_model = vae.VAE()# TODO \n",
    "optimizer = torch.optim.Adam(vae_model.parameters(), lr = LEARNING_RATE, betas=(0.0,0.9))# TODO \n",
    "# add a learning rate scheduler based on the lr_lambda function\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda) # TODO\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/34 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32x16384 and 4096x512)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\marti\\OneDrive - TU Eindhoven\\Documenten\\Master\\Q3\\Capita Selecta\\Project\\CapitaSelectaCode\\Machine learning\\main.ipynb Cell 4\u001b[0m in \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marti/OneDrive%20-%20TU%20Eindhoven/Documenten/Master/Q3/Capita%20Selecta/Project/CapitaSelectaCode/Machine%20learning/main.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m inputs, labels \u001b[39min\u001b[39;00m tqdm(dataloader, position\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marti/OneDrive%20-%20TU%20Eindhoven/Documenten/Master/Q3/Capita%20Selecta/Project/CapitaSelectaCode/Machine%20learning/main.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m# needed to zero gradients in each iterations\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marti/OneDrive%20-%20TU%20Eindhoven/Documenten/Master/Q3/Capita%20Selecta/Project/CapitaSelectaCode/Machine%20learning/main.ipynb#W3sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/marti/OneDrive%20-%20TU%20Eindhoven/Documenten/Master/Q3/Capita%20Selecta/Project/CapitaSelectaCode/Machine%20learning/main.ipynb#W3sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     recons, mu, logvar \u001b[39m=\u001b[39m vae_model(inputs)  \u001b[39m# forward pass\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marti/OneDrive%20-%20TU%20Eindhoven/Documenten/Master/Q3/Capita%20Selecta/Project/CapitaSelectaCode/Machine%20learning/main.ipynb#W3sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     loss \u001b[39m=\u001b[39m vae\u001b[39m.\u001b[39mvae_loss(inputs, recons, mu, logvar)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marti/OneDrive%20-%20TU%20Eindhoven/Documenten/Master/Q3/Capita%20Selecta/Project/CapitaSelectaCode/Machine%20learning/main.ipynb#W3sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()  \u001b[39m# backpropagate loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\marti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\marti\\OneDrive - TU Eindhoven\\Documenten\\Master\\Q3\\Capita Selecta\\Project\\CapitaSelectaCode\\Machine learning\\vae.py:198\u001b[0m, in \u001b[0;36mVAE.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m    181\u001b[0m     \u001b[39m\"\"\"Performs a forwards pass of the VAE and returns the reconstruction\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[39m    and mean + logvar.\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[39m        the log of the variance of the latent distribution\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 198\u001b[0m     mu, logvar \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x)\n\u001b[0;32m    199\u001b[0m     latent_z \u001b[39m=\u001b[39m sample_z(mu, logvar)\n\u001b[0;32m    201\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerator(latent_z), mu, logvar\n",
      "File \u001b[1;32mc:\\Users\\marti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\marti\\OneDrive - TU Eindhoven\\Documenten\\Master\\Q3\\Capita Selecta\\Project\\CapitaSelectaCode\\Machine learning\\vae.py:93\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     91\u001b[0m     x \u001b[39m=\u001b[39m block(x)          \n\u001b[0;32m     92\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool(x) \n\u001b[1;32m---> 93\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout(x)\n\u001b[0;32m     95\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mchunk(x, \u001b[39m2\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\marti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\marti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\marti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\marti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x16384 and 4096x512)"
     ]
    }
   ],
   "source": [
    "x_real = next(iter(valid_dataloader))\n",
    "# training loop\n",
    "writer = SummaryWriter(log_dir=TENSORBOARD_LOGDIR)  # tensorboard summary\n",
    "for epoch in range(N_EPOCHS):\n",
    "    current_train_loss = 0.0\n",
    "    current_valid_loss = 0.0\n",
    "    \n",
    "    # TODO \n",
    "    # training iterations\n",
    "    for inputs, labels in tqdm(dataloader, position=0):\n",
    "        # needed to zero gradients in each iterations\n",
    "        optimizer.zero_grad()\n",
    "        recons, mu, logvar = vae_model(inputs)  # forward pass\n",
    "        loss = vae.vae_loss(inputs, recons, mu, logvar)\n",
    "        loss.backward()  # backpropagate loss\n",
    "        current_train_loss += loss.item()\n",
    "        optimizer.step()  # update weights\n",
    "        \n",
    "    # evaluate validation loss\n",
    "    with torch.no_grad():\n",
    "        vae_model.eval()\n",
    "        for inputs in tqdm(valid_dataloader, position=0):\n",
    "            recons, mu, logvar = vae_model(inputs)   # forward pass\n",
    "            loss = vae.vae_loss(inputs, recons, mu, logvar)\n",
    "            current_valid_loss += loss.item()\n",
    "        \n",
    "        vae_model.train()\n",
    "    # write to tensorboard log\n",
    "    writer.add_scalar(\"Loss/train\", current_train_loss / len(dataloader), epoch)\n",
    "    writer.add_scalar(\n",
    "        \"Loss/validation\", current_valid_loss / len(valid_dataloader), epoch\n",
    "    )\n",
    "    print(f\"Epoch #{epoch} Loss/train {current_train_loss / len(dataloader):.5f} | Loss/validation {current_valid_loss / len(valid_dataloader):.5f}\")\n",
    "    scheduler.step() # step the learning step scheduler\n",
    "\n",
    "    # save examples of real/fake images\n",
    "    if (epoch + 1) % DISPLAY_FREQ == 0:\n",
    "        x_recon = vae_model(x_real)[0]\n",
    "        img_grid = make_grid(\n",
    "            torch.cat((x_recon[:5], x_real[:5])), nrow=5, padding=12, pad_value=-1\n",
    "        )\n",
    "        img = np.clip(img_grid[0][np.newaxis], -1, 1) / 2 + 0.5\n",
    "        writer.add_image(\n",
    "            \"Real_fake\", img, epoch + 1,\n",
    "        )\n",
    "        plt.imsave(PROGRESS_DIR / f\"Real_fake_{epoch:03d}.png\", img[0])\n",
    "    \n",
    "    # TODO: sample noise  \n",
    "    # TODO: generate images and display NEED TO BE ADDED\n",
    "\n",
    "torch.save(vae_model.state_dict(), CHECKPOINTS_DIR / \"vae_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINTS_DIR = WORKING_DIR / \"segmentation_model_weights\"\n",
    "CHECKPOINTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TENSORBOARD_LOGDIR = WORKING_DIR / \"segmentation_runs\"\n",
    "TENSORBOARD_LOGDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# training settings and hyperparameters\n",
    "NO_VALIDATION_PATIENTS = 2\n",
    "IMAGE_SIZE = [64, 64]  # images are made smaller to save training time\n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 100\n",
    "LEARNING_RATE = 1e-4\n",
    "TOLERANCE = 0.01  # for early stopping\n",
    "\n",
    "# find patient folders in training directory\n",
    "# excluding hidden folders (start with .)\n",
    "patients = [\n",
    "    path\n",
    "    for path in DATA_DIR.glob(\"*\")\n",
    "    if not any(part.startswith(\".\") for part in path.parts)\n",
    "]\n",
    "random.shuffle(patients)\n",
    "\n",
    "# split in training/validation after shuffling\n",
    "partition = {\n",
    "    \"train\": patients[:-NO_VALIDATION_PATIENTS],\n",
    "    \"validation\": patients[-NO_VALIDATION_PATIENTS:],\n",
    "}\n",
    "\n",
    "# load training data and create DataLoader with batching and shuffling\n",
    "dataset = utils.ProstateMRDataset(partition[\"train\"], IMAGE_SIZE)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "# load validation data\n",
    "valid_dataset = utils.ProstateMRDataset(partition[\"validation\"], IMAGE_SIZE)\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "# initialise model, optimiser, and loss function\n",
    "loss_function = utils.DiceBCELoss()\n",
    "unet_model = u_net.UNet(num_classes=1)\n",
    "optimizer = torch.optim.Adam(unet_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "minimum_valid_loss = 10  # initial validation loss\n",
    "writer = SummaryWriter(log_dir=TENSORBOARD_LOGDIR)  # tensorboard summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [02:04<00:00,  3.66s/it]\n",
      "100%|██████████| 5/5 [00:07<00:00,  1.59s/it]\n",
      "100%|██████████| 34/34 [01:53<00:00,  3.33s/it]\n",
      "100%|██████████| 5/5 [00:06<00:00,  1.26s/it]\n",
      " 47%|████▋     | 16/34 [01:07<01:20,  4.50s/it]"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for epoch in range(N_EPOCHS):\n",
    "    current_train_loss = 0.0\n",
    "    current_valid_loss = 0.0\n",
    "\n",
    "    # training iterations\n",
    "    # tqdm is for timing iteratiions\n",
    "    for inputs, labels in tqdm(dataloader, position=0):\n",
    "        # needed to zero gradients in each iterations\n",
    "        optimizer.zero_grad()\n",
    "        outputs = unet_model(inputs)  # forward pass\n",
    "        loss = loss_function(outputs, labels.float())\n",
    "        loss.backward()  # backpropagate loss\n",
    "        current_train_loss += loss.item()\n",
    "        optimizer.step()  # update weights\n",
    "\n",
    "    # evaluate validation loss\n",
    "    with torch.no_grad():\n",
    "        unet_model.eval()  # turn off training option for evaluation\n",
    "        for inputs, labels in tqdm(valid_dataloader, position=0):\n",
    "            outputs = unet_model(inputs)  # forward pass\n",
    "            loss = loss_function(outputs, labels.float())\n",
    "            current_valid_loss += loss.item()\n",
    "\n",
    "        unet_model.train()  # turn training back on\n",
    "\n",
    "    # write to tensorboard log\n",
    "    writer.add_scalar(\"Loss/train\", current_train_loss / len(dataloader), epoch)\n",
    "    writer.add_scalar(\n",
    "        \"Loss/validation\", current_valid_loss / len(valid_dataloader), epoch\n",
    "    )\n",
    "\n",
    "    # if validation loss is improving, save model checkpoint\n",
    "    # only start saving after 10 epochs\n",
    "    if (current_valid_loss / len(valid_dataloader)) < minimum_valid_loss + TOLERANCE:\n",
    "        minimum_valid_loss = current_valid_loss / len(valid_dataloader)\n",
    "        if epoch > 9:\n",
    "            torch.save(\n",
    "                unet_model.cpu().state_dict(),\n",
    "                CHECKPOINTS_DIR / f\"u_net_{epoch}.pth\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\..\\..\\Data\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\marti\\OneDrive - TU Eindhoven\\Documenten\\Master\\Q3\\Capita Selecta\\Project\\CapitaSelectaCode\\Machine learning\\main.ipynb Cell 7\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marti/OneDrive%20-%20TU%20Eindhoven/Documenten/Master/Q3/Capita%20Selecta/Project/CapitaSelectaCode/Machine%20learning/main.ipynb#W6sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m partition \u001b[39m=\u001b[39m {\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marti/OneDrive%20-%20TU%20Eindhoven/Documenten/Master/Q3/Capita%20Selecta/Project/CapitaSelectaCode/Machine%20learning/main.ipynb#W6sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m: patients[:\u001b[39m-\u001b[39mNO_VALIDATION_PATIENTS],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marti/OneDrive%20-%20TU%20Eindhoven/Documenten/Master/Q3/Capita%20Selecta/Project/CapitaSelectaCode/Machine%20learning/main.ipynb#W6sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m\"\u001b[39m: patients[\u001b[39m-\u001b[39mNO_VALIDATION_PATIENTS:],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marti/OneDrive%20-%20TU%20Eindhoven/Documenten/Master/Q3/Capita%20Selecta/Project/CapitaSelectaCode/Machine%20learning/main.ipynb#W6sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m }\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marti/OneDrive%20-%20TU%20Eindhoven/Documenten/Master/Q3/Capita%20Selecta/Project/CapitaSelectaCode/Machine%20learning/main.ipynb#W6sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# load validation data\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/marti/OneDrive%20-%20TU%20Eindhoven/Documenten/Master/Q3/Capita%20Selecta/Project/CapitaSelectaCode/Machine%20learning/main.ipynb#W6sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m valid_dataset \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39;49mProstateMRDataset(partition[\u001b[39m\"\u001b[39;49m\u001b[39mvalidation\u001b[39;49m\u001b[39m\"\u001b[39;49m], IMAGE_SIZE)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marti/OneDrive%20-%20TU%20Eindhoven/Documenten/Master/Q3/Capita%20Selecta/Project/CapitaSelectaCode/Machine%20learning/main.ipynb#W6sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m unet_model \u001b[39m=\u001b[39m u_net\u001b[39m.\u001b[39mUNet(num_classes\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marti/OneDrive%20-%20TU%20Eindhoven/Documenten/Master/Q3/Capita%20Selecta/Project/CapitaSelectaCode/Machine%20learning/main.ipynb#W6sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m unet_model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(CHECKPOINTS_DIR))\n",
      "File \u001b[1;32mc:\\Users\\marti\\OneDrive - TU Eindhoven\\Documenten\\Master\\Q3\\Capita Selecta\\Project\\CapitaSelectaCode\\Machine learning\\utils.py:33\u001b[0m, in \u001b[0;36mProstateMRDataset.__init__\u001b[1;34m(self, paths, img_size)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39m# number of patients and slices in the dataset\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mno_patients \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmr_image_list)\n\u001b[1;32m---> 33\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mno_slices \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmr_image_list[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m     35\u001b[0m \u001b[39m# transforms to resize images\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimg_transform \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose(\n\u001b[0;32m     37\u001b[0m     [\n\u001b[0;32m     38\u001b[0m         transforms\u001b[39m.\u001b[39mToPILImage(),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m     ]\n\u001b[0;32m     43\u001b[0m )\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# directorys with data and to stored training checkpoints\n",
    "\n",
    "# this is my best epoch - what is yours?\n",
    "BEST_EPOCH = 30\n",
    "CHECKPOINTS_DIR = Path.cwd() / \"segmentation_model_weights\" / f\"u_net_{BEST_EPOCH}.pth\"\n",
    "\n",
    "# hyperparameters\n",
    "NO_VALIDATION_PATIENTS = 2\n",
    "IMAGE_SIZE = [64, 64]\n",
    "\n",
    "# find patient folders in training directory\n",
    "# excluding hidden folders (start with .)\n",
    "patients = [\n",
    "    path\n",
    "    for path in DATA_DIR.glob(\"*\")\n",
    "    if not any(part.startswith(\".\") for part in path.parts)\n",
    "]\n",
    "random.shuffle(patients)\n",
    "\n",
    "# split in training/validation after shuffling\n",
    "partition = {\n",
    "    \"train\": patients[:-NO_VALIDATION_PATIENTS],\n",
    "    \"validation\": patients[-NO_VALIDATION_PATIENTS:],\n",
    "}\n",
    "\n",
    "# load validation data\n",
    "valid_dataset = utils.ProstateMRDataset(partition[\"validation\"], IMAGE_SIZE)\n",
    "\n",
    "unet_model = u_net.UNet(num_classes=1)\n",
    "unet_model.load_state_dict(torch.load(CHECKPOINTS_DIR))\n",
    "unet_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# apply for all images and compute Dice score with ground-truth.\n",
    "# output .mhd images with the predicted segmentations\n",
    "with torch.no_grad():\n",
    "    predict_index = 75\n",
    "    (input, target) = valid_dataset[predict_index]\n",
    "    output = torch.sigmoid(unet_model(input[np.newaxis, ...]))\n",
    "    prediction = torch.round(output)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3)\n",
    "    ax[0].imshow(input[0], cmap=\"gray\")\n",
    "    ax[0].set_title(\"Input\")\n",
    "    ax[0].axis(\"off\")\n",
    "\n",
    "    ax[1].imshow(target[0])\n",
    "    ax[1].set_title(\"Ground-truth\")\n",
    "    ax[1].axis(\"off\")\n",
    "\n",
    "    ax[2].imshow(prediction[0, 0])\n",
    "    ax[2].set_title(\"Prediction\")\n",
    "    ax[2].axis(\"off\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
