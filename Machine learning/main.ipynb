{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import utils\n",
    "import mask_vae\n",
    "import u_net\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Any, Callable, List, Tuple, Type, Union\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import os\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "from torchvision.utils import make_grid, save_image\n",
    "import copy\n",
    "from torchvision.datasets import FashionMNIST\n",
    "\n",
    "from torchmetrics import StructuralSimilarityIndexMeasure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONV = \\\n",
    "lambda in_channels, out_channels, kernel_size, stride=1, padding=0, bias=True, : \\\n",
    "    nn.Conv2d(in_channels, out_channels, kernel_size, stride  , padding  , bias=bias, padding_mode=\"reflect\")\n",
    "BATCH_NORM = nn.InstanceNorm2d\n",
    "RELU = lambda : nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
    "UPSAMPLE = lambda : Upsample([1,3,3,1])\n",
    "DOWNSAMPLE = lambda : Downsample([1,3,3,1])\n",
    "LAYERS = (1,1,1,1)\n",
    "CHS_E = (32, 64, 128, 256)\n",
    "CHS_D = (256, 128, 64, 32)\n",
    "Z_DIM = 256\n",
    "\n",
    "SSIM = StructuralSimilarityIndexMeasure(data_range=1, kernel_size=7).to(device)\n",
    "LOSS = nn.L1Loss()\n",
    "\n",
    "KLD_EPOCHS = 75\n",
    "\n",
    "WORKING_DIR = Path(r\"C:\\Users\\20182371\\Documents\\TUe\\8DM20_CS_Medical_Imaging\\DeepLearning_Project\")\n",
    "DATA_DIR = '/content/drive/My Drive/Prostate_MRI/'\n",
    "DATA_DIR = WORKING_DIR / \"TrainingData\"\n",
    "CHECKPOINTS_DIR = WORKING_DIR / \"vae_model_weights\"\n",
    "CHECKPOINTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PROGRESS_DIR = WORKING_DIR / \"progress\"\n",
    "PROGRESS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TENSORBOARD_LOGDIR = WORKING_DIR / \"vae_runs\"\n",
    "TENSORBOARD_LOGDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# training settings and hyperparameters\n",
    "NO_VALIDATION_PATIENTS = 3\n",
    "IMAGE_SIZE = [64, 64]\n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 200\n",
    "DECAY_LR_AFTER = 50\n",
    "LEARNING_RATE = 1e-3\n",
    "DISPLAY_FREQ = 5\n",
    "\n",
    "OPTIMIZER = lambda parameters, lr : torch.optim.Adam(parameters, lr=lr, betas=(0.5,0.999))\n",
    "MODE = transforms.InterpolationMode.BILINEAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def lr_lambda(the_epoch):\n",
    "    \"\"\"Function for scheduling learning rate\"\"\"\n",
    "    return (\n",
    "        1.0\n",
    "        if the_epoch < DECAY_LR_AFTER\n",
    "        else 1 - float(the_epoch - DECAY_LR_AFTER) / (N_EPOCHS - DECAY_LR_AFTER)\n",
    "    )\n",
    "\n",
    "# initialise model, optimiser\n",
    "mask_vae_model = mask_vae.VAE(spade=False).to(device)\n",
    "optimizer = OPTIMIZER(mask_vae_model.parameters(), lr = LEARNING_RATE)\n",
    "# optimizer = torch.optim.SGD(vae_model.parameters(), lr = LEARNING_RATE, momentum=0.95, nesterov=True)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "# find patient folders in training directory\n",
    "# excluding hidden folders (start with .)\n",
    "patients = [\n",
    "    path\n",
    "    for path in DATA_DIR.glob(\"*\")\n",
    "    if not any(part.startswith(\".\") for part in path.parts)\n",
    "]\n",
    "random.shuffle(patients)\n",
    "\n",
    "# split in training/validation after shuffling\n",
    "partition = {\n",
    "    \"train\": patients[:-NO_VALIDATION_PATIENTS],\n",
    "    \"validation\": patients[-NO_VALIDATION_PATIENTS:],\n",
    "}\n",
    "\n",
    "# load training data and create DataLoader with batching and shuffling\n",
    "dataset = utils.ProstateMRDataset(partition[\"train\"], IMAGE_SIZE)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "# load validation data\n",
    "valid_dataset = utils.ProstateMRDataset(partition[\"validation\"], IMAGE_SIZE)\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "\n",
    "img = dataset[20][1][0]\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for //: 'list' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_30124\\867607902.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx_real_img_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_real_mask_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mx_real_img_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_real_img_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mx_real_mask_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_real_mask_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mx_real_img_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_real_mask_valid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalid_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\TUe\\CapitaSelectaCode\\Machine learning\\utils.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;31m# compute which slice an index corresponds to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m         \u001b[0mpatient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_slices\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m         \u001b[0mthe_slice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpatient\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_slices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for //: 'list' and 'int'"
     ]
    }
   ],
   "source": [
    "x_real_img_train, x_real_mask_train = dataset[[10,20,50,60,100]]\n",
    "x_real_img_train = x_real_img_train.to(device)\n",
    "x_real_mask_train = x_real_mask_train.to(device)\n",
    "\n",
    "x_real_img_valid, x_real_mask_valid = valid_dataset[[10,20,50,60,100]]\n",
    "x_real_img_valid = x_real_img_valid.to(device)\n",
    "x_real_mask_valid = x_real_mask_valid.to(device)\n",
    "\n",
    "z_fixed = torch.randn(5, Z_DIM).to(device)\n",
    "\n",
    "# training loop\n",
    "best_loss = 1e8\n",
    "best_weights = copy.deepcopy(vae_model.state_dict())\n",
    "\n",
    "kld_epochs = KLD_EPOCHS\n",
    "kld_nsteps = kld_epochs * len(dataloader)\n",
    "nstep = 0\n",
    "kld_weight = torch.sigmoid(torch.tensor(nstep*12/kld_nsteps - 6)).item()\n",
    "ssim_weight = 0.5\n",
    "for epoch in range(N_EPOCHS):\n",
    "    current_train_loss = 0.0\n",
    "    current_valid_loss = 0.0\n",
    "\n",
    "    for images, masks in tqdm(dataloader, position=0): \n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        # p = torch.rand((1,)).item()\n",
    "        # if p > 0.4:\n",
    "        #     inputs = torch.rot90(inputs, dims=(-2,-1))\n",
    "        # if p > 0.6:\n",
    "        #     inputs = torch.rot90(inputs, dims=(-2,-1))\n",
    "        # if p > 0.8:\n",
    "        #     inputs = torch.rot90(inputs, dims=(-2,-1))\n",
    "        mask_vae_model.train()\n",
    "        optimizer.zero_grad()\n",
    "        recons, mu, logvar = vae_model(masks)  # forward pass\n",
    "        # recons = recons/2.0 + 0.5\n",
    "        loss = vae_loss(masks, recons, mu, logvar, kld_weight, ssim_weight)\n",
    "        loss.backward()  # backpropagate loss\n",
    "        current_train_loss += loss.item()\n",
    "        optimizer.step()  # update weights\n",
    "\n",
    "        nstep += 1\n",
    "        kld_weight = torch.sigmoid(torch.tensor(nstep*12/kld_nsteps - 6)).item()\n",
    "        \n",
    "    # evaluate validation loss\n",
    "    with torch.no_grad():\n",
    "        vae_model.eval()\n",
    "        for images, masks in tqdm(valid_dataloader, position=0):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            recons, mu, logvar = vae_model(masks)   # forward pass\n",
    "            # recons = recons/2.0 + 0.5\n",
    "            loss = vae_loss(masks, recons, mu, logvar, 1., ssim_weight)\n",
    "            current_valid_loss += loss.item()\n",
    "\n",
    "    epoch_train_loss = current_train_loss / len(dataloader)\n",
    "    epoch_valid_loss = current_valid_loss / len(valid_dataloader)\n",
    "    \n",
    "    print(f\"Epoch #{epoch} Loss/train {epoch_train_loss:.5f} | Loss/validation {epoch_valid_loss:.5f}\")\n",
    "    if epoch_valid_loss < best_loss:\n",
    "        best_loss = epoch_valid_loss\n",
    "        print(\"saving best model\")\n",
    "        best_weights = copy.deepcopy(vae_model.state_dict())\n",
    "        torch.save(best_weights, CHECKPOINTS_DIR / \"vae_model.pt\")\n",
    "    scheduler.step() # step the learning step scheduler\n",
    "\n",
    "    # save examples of real/fake images\n",
    "    if (epoch + 1) % DISPLAY_FREQ == 0:\n",
    "        with torch.no_grad():\n",
    "            x_recon_train = mask_vae_model(x_real_img_train, x_real_mask_train)[0]\n",
    "            x_recon_valid = mask_vae_model(x_real_img_valid, x_real_mask_valid)[0]\n",
    "            # x_recon = x_recon/2.0 + 0.5\n",
    "            x_rand  = vae_model.generator(z_fixed, x_real_mask_valid)\n",
    "            # x_rand = x_rand/2.0 + 0.5\n",
    "            img_grid = make_grid(\n",
    "                torch.cat([\n",
    "                    x_real_img_train.cpu()[:5], \n",
    "                    x_recon_train.cpu()[:5],\n",
    "                    x_real_img_valid.cpu()[:5], \n",
    "                    x_recon_valid.cpu()[:5], \n",
    "                    x_rand.cpu()[:5],\n",
    "                ]), \n",
    "                nrow=5, \n",
    "                padding=12, \n",
    "                pad_value=-1, \n",
    "            )\n",
    "            save_image(img_grid/2.0+0.5, PROGRESS_DIR / f\"Real_fake_{(epoch+1):03d}.png\")\n",
    "            # img = (img_grid.numpy().transpose(1,2,0) + 1)/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to ensure reproducible training/validation split\n",
    "random.seed(42)\n",
    "\n",
    "# directorys with data and to store training checkpoints and logs\n",
    "WORKING_DIR = Path(r\"C:\\Users\\20182371\\Documents\\TUe\\8DM20_CS_Medical_Imaging\\DeepLearning_Project\")\n",
    "DATA_DIR = WORKING_DIR / \"TrainingData\"\n",
    "CHECKPOINTS_DIR = WORKING_DIR / \"vae_model_weights\"\n",
    "CHECKPOINTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PROGRESS_DIR = WORKING_DIR / \"progress\"\n",
    "PROGRESS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TENSORBOARD_LOGDIR = WORKING_DIR / \"vae_runs\"\n",
    "TENSORBOARD_LOGDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# training settings and hyperparameters\n",
    "NO_VALIDATION_PATIENTS = 2\n",
    "IMAGE_SIZE = [64, 64]\n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 200\n",
    "DECAY_LR_AFTER = 50\n",
    "LEARNING_RATE = 1e-3\n",
    "DISPLAY_FREQ = 5\n",
    "\n",
    "# dimension of VAE latent space\n",
    "Z_DIM = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to reduce the\n",
    "def lr_lambda(the_epoch):\n",
    "    \"\"\"Function for scheduling learning rate\"\"\"\n",
    "    return (\n",
    "        1.0\n",
    "        if the_epoch < DECAY_LR_AFTER\n",
    "        else 1 - float(the_epoch - DECAY_LR_AFTER) / (N_EPOCHS - DECAY_LR_AFTER)\n",
    ")\n",
    "\n",
    "\n",
    "# find patient folders in training directory\n",
    "# excluding hidden folders (start with .)\n",
    "patients = [\n",
    "    path\n",
    "    for path in DATA_DIR.glob(\"*\")\n",
    "    if not any(part.startswith(\".\") for part in path.parts)\n",
    "]\n",
    "random.shuffle(patients)\n",
    "\n",
    "# split in training/validation after shuffling\n",
    "partition = {\n",
    "    \"train\": patients[:-NO_VALIDATION_PATIENTS],\n",
    "    \"validation\": patients[-NO_VALIDATION_PATIENTS:],\n",
    "}\n",
    "\n",
    "# load training data and create DataLoader with batching and shuffling\n",
    "dataset = utils.ProstateMRDataset(partition[\"train\"], IMAGE_SIZE)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "# load validation data\n",
    "valid_dataset = utils.ProstateMRDataset(partition[\"validation\"], IMAGE_SIZE)\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "# initialise model, optimiser\n",
    "vae_model = vae.VAE()# TODO \n",
    "optimizer = torch.optim.Adam(vae_model.parameters(), lr = LEARNING_RATE, betas=(0.0,0.9))# TODO \n",
    "# add a learning rate scheduler based on the lr_lambda function\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda) # TODO\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/34 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32x16384 and 4096x512)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28548\\3106437976.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;31m# needed to zero gradients in each iterations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mrecons\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvae_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvae_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecons\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# backpropagate loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\8DM20\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\TUe\\CapitaSelectaCode\\Machine learning\\vae.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    196\u001b[0m             \u001b[0mthe\u001b[0m \u001b[0mlog\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mvariance\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlatent\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \"\"\"\n\u001b[1;32m--> 198\u001b[1;33m         \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m         \u001b[0mlatent_z\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_z\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\8DM20\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\TUe\\CapitaSelectaCode\\Machine learning\\vae.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     91\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 2 chunks, 1 each for mu and logvar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\8DM20\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\8DM20\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\8DM20\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\8DM20\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x16384 and 4096x512)"
     ]
    }
   ],
   "source": [
    "x_real = next(iter(valid_dataloader))\n",
    "# training loop\n",
    "writer = SummaryWriter(log_dir=TENSORBOARD_LOGDIR)  # tensorboard summary\n",
    "for epoch in range(N_EPOCHS):\n",
    "    current_train_loss = 0.0\n",
    "    current_valid_loss = 0.0\n",
    "    \n",
    "    # TODO \n",
    "    # training iterations\n",
    "    for inputs, labels in tqdm(dataloader, position=0):\n",
    "        # needed to zero gradients in each iterations\n",
    "        optimizer.zero_grad()\n",
    "        recons, mu, logvar = vae_model(inputs)  # forward pass\n",
    "        loss = vae.vae_loss(inputs, recons, mu, logvar)\n",
    "        loss.backward()  # backpropagate loss\n",
    "        current_train_loss += loss.item()\n",
    "        optimizer.step()  # update weights\n",
    "        \n",
    "    # evaluate validation loss\n",
    "    with torch.no_grad():\n",
    "        vae_model.eval()\n",
    "        for inputs in tqdm(valid_dataloader, position=0):\n",
    "            recons, mu, logvar = vae_model(inputs)   # forward pass\n",
    "            loss = vae.vae_loss(inputs, recons, mu, logvar)\n",
    "            current_valid_loss += loss.item()\n",
    "        \n",
    "        vae_model.train()\n",
    "    # write to tensorboard log\n",
    "    writer.add_scalar(\"Loss/train\", current_train_loss / len(dataloader), epoch)\n",
    "    writer.add_scalar(\n",
    "        \"Loss/validation\", current_valid_loss / len(valid_dataloader), epoch\n",
    "    )\n",
    "    print(f\"Epoch #{epoch} Loss/train {current_train_loss / len(dataloader):.5f} | Loss/validation {current_valid_loss / len(valid_dataloader):.5f}\")\n",
    "    scheduler.step() # step the learning step scheduler\n",
    "\n",
    "    # save examples of real/fake images\n",
    "    if (epoch + 1) % DISPLAY_FREQ == 0:\n",
    "        x_recon = vae_model(x_real)[0]\n",
    "        img_grid = make_grid(\n",
    "            torch.cat((x_recon[:5], x_real[:5])), nrow=5, padding=12, pad_value=-1\n",
    "        )\n",
    "        img = np.clip(img_grid[0][np.newaxis], -1, 1) / 2 + 0.5\n",
    "        writer.add_image(\n",
    "            \"Real_fake\", img, epoch + 1,\n",
    "        )\n",
    "        plt.imsave(PROGRESS_DIR / f\"Real_fake_{epoch:03d}.png\", img[0])\n",
    "    \n",
    "    # TODO: sample noise  \n",
    "    # TODO: generate images and display NEED TO BE ADDED\n",
    "\n",
    "torch.save(vae_model.state_dict(), CHECKPOINTS_DIR / \"vae_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINTS_DIR = WORKING_DIR / \"segmentation_model_weights\"\n",
    "CHECKPOINTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TENSORBOARD_LOGDIR = WORKING_DIR / \"segmentation_runs\"\n",
    "TENSORBOARD_LOGDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# training settings and hyperparameters\n",
    "NO_VALIDATION_PATIENTS = 2\n",
    "IMAGE_SIZE = [64, 64]  # images are made smaller to save training time\n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 100\n",
    "LEARNING_RATE = 1e-4\n",
    "TOLERANCE = 0.01  # for early stopping\n",
    "\n",
    "# find patient folders in training directory\n",
    "# excluding hidden folders (start with .)\n",
    "patients = [\n",
    "    path\n",
    "    for path in DATA_DIR.glob(\"*\")\n",
    "    if not any(part.startswith(\".\") for part in path.parts)\n",
    "]\n",
    "random.shuffle(patients)\n",
    "\n",
    "# split in training/validation after shuffling\n",
    "partition = {\n",
    "    \"train\": patients[:-NO_VALIDATION_PATIENTS],\n",
    "    \"validation\": patients[-NO_VALIDATION_PATIENTS:],\n",
    "}\n",
    "\n",
    "# load training data and create DataLoader with batching and shuffling\n",
    "dataset = utils.ProstateMRDataset(partition[\"train\"], IMAGE_SIZE)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "# load validation data\n",
    "valid_dataset = utils.ProstateMRDataset(partition[\"validation\"], IMAGE_SIZE)\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "# initialise model, optimiser, and loss function\n",
    "loss_function = utils.DiceBCELoss()\n",
    "unet_model = u_net.UNet(num_classes=1)\n",
    "optimizer = torch.optim.Adam(unet_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "minimum_valid_loss = 10  # initial validation loss\n",
    "writer = SummaryWriter(log_dir=TENSORBOARD_LOGDIR)  # tensorboard summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [02:04<00:00,  3.66s/it]\n",
      "100%|██████████| 5/5 [00:07<00:00,  1.59s/it]\n",
      "100%|██████████| 34/34 [01:53<00:00,  3.33s/it]\n",
      "100%|██████████| 5/5 [00:06<00:00,  1.26s/it]\n",
      " 47%|████▋     | 16/34 [01:07<01:20,  4.50s/it]"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for epoch in range(N_EPOCHS):\n",
    "    current_train_loss = 0.0\n",
    "    current_valid_loss = 0.0\n",
    "\n",
    "    # training iterations\n",
    "    # tqdm is for timing iteratiions\n",
    "    for inputs, labels in tqdm(dataloader, position=0):\n",
    "        # needed to zero gradients in each iterations\n",
    "        optimizer.zero_grad()\n",
    "        outputs = unet_model(inputs)  # forward pass\n",
    "        loss = loss_function(outputs, labels.float())\n",
    "        loss.backward()  # backpropagate loss\n",
    "        current_train_loss += loss.item()\n",
    "        optimizer.step()  # update weights\n",
    "\n",
    "    # evaluate validation loss\n",
    "    with torch.no_grad():\n",
    "        unet_model.eval()  # turn off training option for evaluation\n",
    "        for inputs, labels in tqdm(valid_dataloader, position=0):\n",
    "            outputs = unet_model(inputs)  # forward pass\n",
    "            loss = loss_function(outputs, labels.float())\n",
    "            current_valid_loss += loss.item()\n",
    "\n",
    "        unet_model.train()  # turn training back on\n",
    "\n",
    "    # write to tensorboard log\n",
    "    writer.add_scalar(\"Loss/train\", current_train_loss / len(dataloader), epoch)\n",
    "    writer.add_scalar(\n",
    "        \"Loss/validation\", current_valid_loss / len(valid_dataloader), epoch\n",
    "    )\n",
    "\n",
    "    # if validation loss is improving, save model checkpoint\n",
    "    # only start saving after 10 epochs\n",
    "    if (current_valid_loss / len(valid_dataloader)) < minimum_valid_loss + TOLERANCE:\n",
    "        minimum_valid_loss = current_valid_loss / len(valid_dataloader)\n",
    "        if epoch > 9:\n",
    "            torch.save(\n",
    "                unet_model.cpu().state_dict(),\n",
    "                CHECKPOINTS_DIR / f\"u_net_{epoch}.pth\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\..\\..\\Data\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\marti\\OneDrive - TU Eindhoven\\Documenten\\Master\\Q3\\Capita Selecta\\Project\\CapitaSelectaCode\\Machine learning\\main.ipynb Cell 7\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marti/OneDrive%20-%20TU%20Eindhoven/Documenten/Master/Q3/Capita%20Selecta/Project/CapitaSelectaCode/Machine%20learning/main.ipynb#W6sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m partition \u001b[39m=\u001b[39m {\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marti/OneDrive%20-%20TU%20Eindhoven/Documenten/Master/Q3/Capita%20Selecta/Project/CapitaSelectaCode/Machine%20learning/main.ipynb#W6sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m: patients[:\u001b[39m-\u001b[39mNO_VALIDATION_PATIENTS],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marti/OneDrive%20-%20TU%20Eindhoven/Documenten/Master/Q3/Capita%20Selecta/Project/CapitaSelectaCode/Machine%20learning/main.ipynb#W6sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m\"\u001b[39m: patients[\u001b[39m-\u001b[39mNO_VALIDATION_PATIENTS:],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marti/OneDrive%20-%20TU%20Eindhoven/Documenten/Master/Q3/Capita%20Selecta/Project/CapitaSelectaCode/Machine%20learning/main.ipynb#W6sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m }\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marti/OneDrive%20-%20TU%20Eindhoven/Documenten/Master/Q3/Capita%20Selecta/Project/CapitaSelectaCode/Machine%20learning/main.ipynb#W6sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# load validation data\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/marti/OneDrive%20-%20TU%20Eindhoven/Documenten/Master/Q3/Capita%20Selecta/Project/CapitaSelectaCode/Machine%20learning/main.ipynb#W6sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m valid_dataset \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39;49mProstateMRDataset(partition[\u001b[39m\"\u001b[39;49m\u001b[39mvalidation\u001b[39;49m\u001b[39m\"\u001b[39;49m], IMAGE_SIZE)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marti/OneDrive%20-%20TU%20Eindhoven/Documenten/Master/Q3/Capita%20Selecta/Project/CapitaSelectaCode/Machine%20learning/main.ipynb#W6sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m unet_model \u001b[39m=\u001b[39m u_net\u001b[39m.\u001b[39mUNet(num_classes\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marti/OneDrive%20-%20TU%20Eindhoven/Documenten/Master/Q3/Capita%20Selecta/Project/CapitaSelectaCode/Machine%20learning/main.ipynb#W6sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m unet_model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(CHECKPOINTS_DIR))\n",
      "File \u001b[1;32mc:\\Users\\marti\\OneDrive - TU Eindhoven\\Documenten\\Master\\Q3\\Capita Selecta\\Project\\CapitaSelectaCode\\Machine learning\\utils.py:33\u001b[0m, in \u001b[0;36mProstateMRDataset.__init__\u001b[1;34m(self, paths, img_size)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39m# number of patients and slices in the dataset\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mno_patients \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmr_image_list)\n\u001b[1;32m---> 33\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mno_slices \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmr_image_list[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m     35\u001b[0m \u001b[39m# transforms to resize images\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimg_transform \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose(\n\u001b[0;32m     37\u001b[0m     [\n\u001b[0;32m     38\u001b[0m         transforms\u001b[39m.\u001b[39mToPILImage(),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m     ]\n\u001b[0;32m     43\u001b[0m )\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# directorys with data and to stored training checkpoints\n",
    "\n",
    "# this is my best epoch - what is yours?\n",
    "BEST_EPOCH = 30\n",
    "CHECKPOINTS_DIR = Path.cwd() / \"segmentation_model_weights\" / f\"u_net_{BEST_EPOCH}.pth\"\n",
    "\n",
    "# hyperparameters\n",
    "NO_VALIDATION_PATIENTS = 2\n",
    "IMAGE_SIZE = [64, 64]\n",
    "\n",
    "# find patient folders in training directory\n",
    "# excluding hidden folders (start with .)\n",
    "patients = [\n",
    "    path\n",
    "    for path in DATA_DIR.glob(\"*\")\n",
    "    if not any(part.startswith(\".\") for part in path.parts)\n",
    "]\n",
    "random.shuffle(patients)\n",
    "\n",
    "# split in training/validation after shuffling\n",
    "partition = {\n",
    "    \"train\": patients[:-NO_VALIDATION_PATIENTS],\n",
    "    \"validation\": patients[-NO_VALIDATION_PATIENTS:],\n",
    "}\n",
    "\n",
    "# load validation data\n",
    "valid_dataset = utils.ProstateMRDataset(partition[\"validation\"], IMAGE_SIZE)\n",
    "\n",
    "unet_model = u_net.UNet(num_classes=1)\n",
    "unet_model.load_state_dict(torch.load(CHECKPOINTS_DIR))\n",
    "unet_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# apply for all images and compute Dice score with ground-truth.\n",
    "# output .mhd images with the predicted segmentations\n",
    "with torch.no_grad():\n",
    "    predict_index = 75\n",
    "    (input, target) = valid_dataset[predict_index]\n",
    "    output = torch.sigmoid(unet_model(input[np.newaxis, ...]))\n",
    "    prediction = torch.round(output)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3)\n",
    "    ax[0].imshow(input[0], cmap=\"gray\")\n",
    "    ax[0].set_title(\"Input\")\n",
    "    ax[0].axis(\"off\")\n",
    "\n",
    "    ax[1].imshow(target[0])\n",
    "    ax[1].set_title(\"Ground-truth\")\n",
    "    ax[1].axis(\"off\")\n",
    "\n",
    "    ax[2].imshow(prediction[0, 0])\n",
    "    ax[2].set_title(\"Prediction\")\n",
    "    ax[2].axis(\"off\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
